# Self-Hosted Supabase Integration for IFC Pipeline Workers

This plan integrates a self-hosted Supabase instance directly into your existing Docker setup, enabling each worker to connect independently to the Supabase database.

## 1. Setting Up Self-Hosted Supabase

### 1.1. Clone the Supabase Repository

```bash
# Clone the Supabase repository
git clone --depth 1 https://github.com/supabase/supabase
cd supabase/docker
```

### 1.2. Configure Environment Variables

Copy the example environment variables and modify them:

```bash
cp .env.example .env
```

Edit the `.env` file to configure your Supabase instance with secure credentials:

```bash
# Important security credentials to update
POSTGRES_PASSWORD=<strong-postgres-password>
JWT_SECRET=<generate-a-secure-jwt-secret>
DASHBOARD_USERNAME=<admin-username>
DASHBOARD_PASSWORD=<strong-admin-password>
SITE_URL=http://localhost:8000

# For connecting from other containers
SUPABASE_PUBLIC_URL=http://supabase-kong:8000
POSTGRES_HOST=supabase-db
POSTGRES_PORT=5432
POOLER_TENANT_ID=postgres
```

### 1.3. Add Supabase to Your Docker Network

Create a new `docker-compose.supabase.yml` file in your project root to include Supabase services in your existing network:

```yaml
version: '3.8'

networks:
  ifc-pipeline-network:
    external: true  # Connect to your existing network

services:
  # Import Supabase services using the existing docker-compose.yml from Supabase
  # but override network settings to connect to your network
  supabase-db:
    extends:
      file: ./supabase/docker/docker-compose.yml
      service: db
    networks:
      - ifc-pipeline-network
  
  supabase-kong:
    extends:
      file: ./supabase/docker/docker-compose.yml
      service: kong
    networks:
      - ifc-pipeline-network
    ports:
      - "8000:8000"  # API Gateway
  
  supabase-auth:
    extends:
      file: ./supabase/docker/docker-compose.yml
      service: auth
    networks:
      - ifc-pipeline-network
  
  supabase-rest:
    extends:
      file: ./supabase/docker/docker-compose.yml
      service: rest
    networks:
      - ifc-pipeline-network
  
  supabase-realtime:
    extends:
      file: ./supabase/docker/docker-compose.yml
      service: realtime
    networks:
      - ifc-pipeline-network
  
  supabase-storage:
    extends:
      file: ./supabase/docker/docker-compose.yml
      service: storage
    networks:
      - ifc-pipeline-network
  
  supabase-meta:
    extends:
      file: ./supabase/docker/docker-compose.yml
      service: meta
    networks:
      - ifc-pipeline-network
  
  supabase-functions:
    extends:
      file: ./supabase/docker/docker-compose.yml
      service: functions
    networks:
      - ifc-pipeline-network
  
  supabase-imgproxy:
    extends:
      file: ./supabase/docker/docker-compose.yml
      service: imgproxy
    networks:
      - ifc-pipeline-network
  
  supabase-analytics:
    extends:
      file: ./supabase/docker/docker-compose.yml
      service: analytics
    networks:
      - ifc-pipeline-network
  
  supabase-studio:
    extends:
      file: ./supabase/docker/docker-compose.yml
      service: studio
    networks:
      - ifc-pipeline-network
  
  supabase-supavisor:
    extends:
      file: ./supabase/docker/docker-compose.yml
      service: supavisor
    networks:
      - ifc-pipeline-network
```

### 1.4. Update Your Main Docker Compose File

Ensure your main `docker-compose.yml` defines the network that will be shared with Supabase:

```yaml
networks:
  ifc-pipeline-network:
    name: ifc-pipeline-network
    driver: bridge
```

### 1.5. Start the Supabase Services

```bash
# First, ensure your ifc-pipeline network exists
docker network create ifc-pipeline-network

# Start your IFC Pipeline services
docker-compose up -d

# Start Supabase services
docker-compose -f docker-compose.supabase.yml up -d
```

### 1.6. Access Supabase Studio

After all services are up and running, you can access Supabase Studio at `http://localhost:8000`. Use the credentials you set in the `.env` file.

## 2. Setting Up Database Tables

Log in to Supabase Studio and create the database tables for your IFC pipeline:

### 2.1. Clash Detection Results Table

Use the SQL Editor in Supabase Studio to execute the following SQL:

```sql
-- Create clash results table
CREATE TABLE clash_results (
  id BIGINT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
  original_clash_id BIGINT REFERENCES clash_results(id),
  clash_set_name TEXT NOT NULL,
  output_filename TEXT NOT NULL,
  clash_count INTEGER NOT NULL,
  clash_data JSONB NOT NULL,
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- Create indexes for faster searching
CREATE INDEX idx_clash_results_original_clash_id ON clash_results(original_clash_id);
CREATE INDEX idx_clash_results_clash_set_name ON clash_results(clash_set_name);
CREATE INDEX idx_clash_results_created_at ON clash_results(created_at);
CREATE INDEX idx_clash_data ON clash_results USING gin (clash_data);

-- Set up RLS (Row Level Security) policies
ALTER TABLE clash_results ENABLE ROW LEVEL SECURITY;

-- Create a policy that allows service role to insert data
CREATE POLICY "Service can insert clash results"
  ON clash_results
  FOR INSERT
  TO authenticated
  WITH CHECK (true);

-- Create a policy that allows anyone to view clash results
CREATE POLICY "Public can view clash results"
  ON clash_results
  FOR SELECT
  TO anon
  USING (true);
```

## 3. Update ifcclash-worker to Connect to Self-hosted Supabase

### 3.1. Update ifcclash-worker Dockerfile

Modify the Dockerfile to include the Supabase client:

```dockerfile
# Add to existing ifcclash-worker/Dockerfile
RUN pip install supabase
```

### 3.2. Create Supabase Client Module

Create a new file `ifcclash-worker/supabase_client.py` modified to work with self-hosted Supabase:

```python
import os
import logging
from supabase import create_client, Client

logger = logging.getLogger(__name__)

# Self-hosted Supabase connection details from environment variables
SUPABASE_URL = os.environ.get("SUPABASE_URL", "http://supabase-kong:8000")
SUPABASE_KEY = os.environ.get("SUPABASE_SERVICE_KEY")  # Use service key for write access

def get_supabase_client() -> Client:
    """Get a connection to the Supabase client"""
    if not SUPABASE_URL or not SUPABASE_KEY:
        logger.warning("Supabase URL or Key not set in environment variables. Database storage disabled.")
        return None
    
    try:
        return create_client(SUPABASE_URL, SUPABASE_KEY)
    except Exception as e:
        logger.error(f"Error creating Supabase client: {str(e)}")
        return None

def save_clash_result(clash_set_name, output_filename, clash_count, clash_data, original_clash_id=None):
    """
    Save clash detection results to Supabase
    
    Args:
        clash_set_name: Name of the clash set
        output_filename: Path to output JSON file
        clash_count: Number of clashes detected
        clash_data: JSON data containing clash results
        original_clash_id: ID of the original clash result (for versioning)
        
    Returns:
        dict: The newly inserted record or None if insert failed
    """
    supabase = get_supabase_client()
    if not supabase:
        logger.warning("Supabase client not available. Skipping database storage.")
        return None
    
    try:
        # Prepare data for insertion
        data = {
            "clash_set_name": clash_set_name,
            "output_filename": output_filename,
            "clash_count": clash_count,
            "clash_data": clash_data,
        }
        
        # Add original_clash_id if it exists
        if original_clash_id:
            data["original_clash_id"] = original_clash_id
            
        # Insert data and return the result
        response = supabase.table("clash_results").insert(data).execute()
        
        if not response.data:
            logger.error("No data returned from Supabase insert")
            return None
            
        logger.info(f"Successfully saved clash result to Supabase with ID: {response.data[0]['id']}")
        return response.data[0]
            
    except Exception as e:
        logger.error(f"Error saving clash result to Supabase: {str(e)}", exc_info=True)
        # Don't raise the exception to avoid failing the job
        return None
```

### 3.3. Update Tasks Implementation

The code remains the same as in the original plan:

```python
# Add this import at the top of the file
from supabase_client import save_clash_result

# Modify the try/except block near the end of run_ifcclash_detection to save results before returning
# Around line 150 in the current implementation

try:
    with open(output_path, 'r') as json_file:
        clash_results = json.load(json_file)
    
    # Count clashes
    clash_count = 0
    clash_set_names = []
    for clash_set in clash_results:
        clash_count += len(clash_set.get("clashes", {}))
        clash_set_names.append(clash_set.get("name", "Unnamed"))
    
    # Create a comma-separated string of clash set names
    clash_set_name = ", ".join(clash_set_names)
    
    # Save to Supabase
    logger.info("Saving clash result to Supabase database")
    db_record = save_clash_result(
        clash_set_name=clash_set_name,
        output_filename=output_path,
        clash_count=clash_count,
        clash_data=clash_results,
        original_clash_id=None  # Set to None for new clash sets
    )
    
    # Return the results (include db_id if available)
    result = {
        "success": True,
        "result": clash_results,
        "clash_count": clash_count,
        "output_path": output_path
    }
    
    # Add database ID if available
    if db_record and 'id' in db_record:
        result["db_id"] = db_record["id"]
    
    return result
```

### 3.4. Update Docker Compose Configuration

Update the environment variables for the ifcclash-worker service in your main `docker-compose.yml`:

```yaml
ifcclash-worker:
  build:
    context: .
    dockerfile: ifcclash-worker/Dockerfile
  volumes:
    - ./shared/uploads:/uploads
    - ./shared/output:/output
    - ./shared/examples:/examples
  environment:
    - PYTHONUNBUFFERED=1
    - LOG_LEVEL=DEBUG
    - REDIS_URL=redis://redis:6379/0
    - SUPABASE_URL=http://supabase-kong:8000
    - SUPABASE_SERVICE_KEY=${SUPABASE_SERVICE_KEY}
  depends_on:
    - redis
  networks:
    - ifc-pipeline-network
  restart: unless-stopped
  deploy:
    replicas: 1
    resources:
      limits:
        cpus: '4.0'
        memory: '6G'
```

### 3.5. Generate and Set API Keys

After starting your Supabase instance, navigate to the Supabase Studio's Project settings > API section to obtain your:

1. `anon` public key
2. `service_role` secret key

Add these to your `.env` file:

```
# Supabase Keys
SUPABASE_ANON_KEY=your-anon-key
SUPABASE_SERVICE_KEY=your-service-role-key
```

## 4. Managing Self-hosted Supabase

### 4.1. Regular Maintenance

Ensure your Supabase instance remains up-to-date and secure:

```bash
# Pull latest Supabase images
cd supabase/docker
git pull
docker-compose pull

# Stop and restart services to update
docker-compose down
docker-compose up -d
```

### 4.2. Backup Strategy

Set up regular PostgreSQL backups:

```bash
# Create a backup script
cat > backup-supabase-db.sh << 'EOF'
#!/bin/bash
BACKUP_DIR="/path/to/backups"
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
CONTAINER_NAME="supabase-db"
mkdir -p $BACKUP_DIR

docker exec $CONTAINER_NAME pg_dump -U postgres -d postgres -F c > $BACKUP_DIR/supabase_backup_$TIMESTAMP.dump
EOF

# Make the script executable
chmod +x backup-supabase-db.sh

# Add to crontab for daily backups
crontab -e
# Add the following line:
# 0 2 * * * /path/to/backup-supabase-db.sh
```

### 4.3. Storage Bucket Setup

Create a storage bucket for clash files:

1. Access Supabase Studio
2. Navigate to Storage section
3. Create a new bucket called `clash-results`
4. Set appropriate permissions for the bucket

## 5. Implementation for Other Workers

The same pattern can be applied to other workers. For each worker:

1. Update the Dockerfile to include the Supabase client
2. Create a Supabase client module
3. Update the tasks implementation to save results to Supabase
4. Create the necessary tables in Supabase

### 5.1. Example Tables for Other Workers

```sql
-- Table for ifcconvert-worker
CREATE TABLE conversion_results (
  id BIGINT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
  input_filename TEXT NOT NULL,
  output_filename TEXT NOT NULL,
  conversion_options JSONB NOT NULL,
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- Table for ifctester-worker
CREATE TABLE tester_results (
  id BIGINT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
  ifc_filename TEXT NOT NULL,
  ids_filename TEXT NOT NULL,
  test_results JSONB NOT NULL,
  pass_count INTEGER NOT NULL,
  fail_count INTEGER NOT NULL,
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- Table for ifcdiff-worker
CREATE TABLE diff_results (
  id BIGINT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
  old_file TEXT NOT NULL,
  new_file TEXT NOT NULL,
  diff_count INTEGER NOT NULL,
  diff_data JSONB NOT NULL,
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);
```

## 6. Potential Challenges with Self-hosted Supabase

### 6.1. Resource Requirements

Self-hosted Supabase requires sufficient CPU, memory, and disk space:

- Minimum recommended: 4 CPU cores, 8GB RAM, 20GB disk
- For production: 8 CPU cores, 16GB RAM, 100GB+ disk

### 6.2. Network Configuration

Ensure proper network configuration between IFC Pipeline containers and Supabase services:

- All services should be on the same Docker network
- No conflicting ports between IFC Pipeline and Supabase
- Proper DNS resolution between containers

### 6.3. Security Considerations

- Change all default passwords and secrets in the Supabase `.env` file
- Implement regular security updates
- Backup your database regularly
- Consider restricting access to Supabase Studio in production

## 7. Benefits of Self-hosted Supabase

1. **Complete Data Control**: All data remains within your infrastructure
2. **No Usage Limits**: Not bound by cloud provider limits or pricing tiers
3. **Customization**: Modify Supabase components as needed
4. **Integration with Existing Infrastructure**: Direct connection to your existing Docker setup
5. **No External Dependencies**: Works in air-gapped environments

## 8. Future Enhancements

1. **Worker-specific Schemas**: Organize tables by worker type with separate schemas
2. **Version Control**: Add support for tracking changes to clash sets over time
3. **Analytics Functions**: Create SQL functions for common analytics queries
4. **Real-time Notifications**: Use Supabase's real-time features to notify about new results
5. **CRON Jobs**: Set up scheduled jobs to clean up old data or generate reports
6. **Monitoring**: Add monitoring for Supabase services using Prometheus and Grafana 